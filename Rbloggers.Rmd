---
title: "R Blogger Practice"
output: html_notebook
---

Logistic Regression and the stock market. 

May 14, 2017
By Data Scientist PakinJa

We will develop a logistic regression example. The exercise was originally published in “An Introduction to Statistical Learning. With applications in R” by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. Springer 2015.

The example we will develop is about predicting when the market value will rise (UP) or fall (Down).

We will carry out the exercise verbatim as published in the aforementioned reference and only with slight changes in the coding style.

For more details on the models, algorithms and parameters interpretation, it is recommended to check the aforementioned reference or any other bibliography of your choice.

  “An Introduction to Statistical Learning.
  With applications in R” by Gareth James,
  Daniela Witten, Trevor Hastie and Robert Tibshirani.
  Springer 2015.

  install and load required packages

```{r}
library(ISLR)
library(psych)
library(knitr)
```
  explore the dataset

Smarket
```{r}
names(Smarket)
dim(Smarket)
summary(Smarket)
kable(head(Smarket))
```

  correlation matrix
```{r}
cor(Smarket[,-9])
```

correlations between th lag variables and today returns are close to zero
the only substantial correlation is between Year and Volume.


```{r}
plot(Smarket$Volume, main = "Stock Market Data", ylab = "Volume", col = "blue")

```
scatterplots, distributions and correlations

```{r}
pairs.panels(Smarket)
```
fit a logistic regression model to predict $Direction using $Lag1 through $Lag5 and $Volume glm(): generalized linear model function family=binomial => logistic regression

```{r}
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
               data = Smarket, family = binomial)
summary(glm.fit)

```
the smallest p_value is associated with Lag1 the negative coefficient for this predictor suggests **that if the market had a positive return yesterday, then it is less likely to go up today at a value of 0.15, the p-value is still relatively large, and so there is no clear evidence of a real association between $Lag1 and $Direction**

explore fitted model coefficients

```{r}
coef(glm.fit)
summary(glm.fit)$coef
summary(glm.fit)$coef[ ,4]
```
**predict the probability that the market will go up, given values of the predictors**
```{r}
glm.probs <- predict(glm.fit, type = "response")
glm.probs[1:10]
contrasts(Smarket$Direction)
```
These values correspond to the probability of the marketgoing up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.

Create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than 0.5.

```{r}
glm.pred <- rep ("Down", 1250)
glm.pred[glm.probs > .5] <- "Up"
```

Confusion matrix in order to determine how many observations were correctly or incorrectly classified. 

```{r}
table(glm.pred, Smarket$Direction)
mean(glm.pred == Smarket$Direction)
```
Model correctly predicted that the market would go up on 507 days and that it would go down on 145 days, for a total of 507 + 145 = 652 correct predictions. Logistic regression correctly predicted the movement of the market 52.2 % of the time. 

To better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data. 

```{r}
train <- (Smarket$Year < 2005)
Smarket.2005 <- Smarket[!train, ]
dim(Smarket.2005)
Direction.2005 <- Smarket$Direction[!train]
Direction.2005
```

```{r}
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
               data = Smarket, family = binomial, subset = train)
glm.fit
glm.probs <- predict(glm.fit, Smarket.2005, type = "response")
```
Compute the predictions for 2005 and compare them to the actual movements of the market over that time period. 

```{r}
glm.pred <- rep("Down", 252)
glm.pred
glm.pred[glm.probs > 0.5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
```

Not generally expect to be able to use previous days returns to predict future market performance. 


Refit the logistic regression using just $Lag1 and $Lag2, which seemed to have the highest predictive power in the original logistic regression model. 

```{r}
glm.fit <- glm(Direction ~ Lag1 + Lag2 , data = Smarket,
               family = binomial, subset = train)
glm.probs <- predict(glm.fit, Smarket.2005 , type = "response")
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > 0.5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
```
Results appear to be a little better: 56%
If we want to predict the returns associated with particular values of $Lag1 and $Lag2

```{r}
predict(glm.fit, newdata = data.frame(Lag1 = c (1.2 ,1.5),
 Lag2 = c(1.1, -0.8)) , type = "response")
```


###Predicting Medical Expenses
library(psych)

Read and explore the data

```{r}
insurance <- read.csv("insurance.csv", header = T)
head(insurance)
str(insurance)
```
Model dependent variable: $expenses
```{r}
### change $charges name to $expenses
colnames(insurance)[7] <- "expenses"
summary(insurance$expenses)
```
```{r}
hist(insurance$expenses, main = "Insurance Expenses", col = "red",
     xlab = "Expenses (USD")
```

```{r}
### explore $region
table(insurance$region)
```
Exploring relationships among features

```{r}
### correlation matrix
cor(insurance[c("age", "bmi", "children", "expenses")])
```

Visualizing relationships among features.

```{r}
### scatterplot matrix
pairs(insurance[c("age", "bmi", "children", "expenses")])
```

```{r}
### scatterplots, distributions and correlations
pairs.panels(insurance[c("age", "bmi", "children", "expenses")])
```

```{r}
### training a model on the data
ins_model <- lm(expenses ~ age + children + bmi + sex + smoker + region, data = insurance)
### this does the same
#ins_model <- lm(expenses ~ ., data = insurance)
### explore model parameters
ins_model

```

```{r}
### evaluating model performance
summary(ins_model)
```
The model explains 74.9% of the variation of the dependent variable (adjusted R-squared: 0.7494).

Improving model performance

```{r}
### adding non-linear relationships
### adding second order term on $age
insurance$age2 <- insurance$age^2
```

Converting a numeric variable to a binary indicator

### $bmi feature only have impact above some value

```{r}
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
```

Putting it all together

```{r}
### improved regression model
ins_model2 <- lm(expenses ~ age + age2 + children + bmi + sex + bmi30*smoker + region, data = insurance)
summary(ins_model2)

```


The accuracy of the model has improved to an 86.5% of explanation of the variation of the independent variable.

###K-Means

The iris dataset contains data about sepal length, sepal width, petal length, and petal width of flowers of different species. Let us see what it looks like:


```{r}
library(datasets)
head(iris)
```

After a little bit of exploration, I found that Petal.Length and Petal.Width were similar among the same species but varied considerably between different species, as demonstrated below:

```{r}
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
```


Clustering

Okay, now that we have seen the data, let us try to cluster it. Since the initial cluster assignments are random, let us set the seed to ensure reproducibility.

```{r}
set.seed(20)
irisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)
irisCluster
```


Since we know that there are 3 species involved, we ask the algorithm to group the data into 3 clusters, and since the starting assignments are random, we specify nstart = 20. This means that R will try 20 different random starting assignments and then select the one with the lowest within cluster variation.
We can see the cluster centroids, the clusters that each data point was assigned to, and the within cluster variation.

Let us compare the clusters with the species.

```{r}
table(irisCluster$cluster, iris$Species)
```

As we can see, the data belonging to the setosa species got grouped into cluster 3, versicolor into cluster 2, and virginica into cluster 1. The algorithm wrongly classified two data points belonging to versicolor and six data points belonging to virginica.

We can also plot the data to see the clusters:

```{r}
irisCluster$cluster <- as.factor(irisCluster$cluster)

ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()
```

That brings us to the end of the article. I hope you enjoyed it! If you have any questions or feedback, feel free to leave a comment or reach out to me on Twitter.

[7 Important Visualizations](https://www.r-bloggers.com/7-visualizations-you-should-learn-in-r/)

```{r}
bigMart <- read.csv("BigMartData.csv", header = T,stringsAsFactors = F)
bigMart
str(bigMart)
```

1. Scatter Plot to see the relationship between variables

```{r, echo=TRUE}
ggplot(bigMart, aes(Item_Visibility, Item_MRP, group = Item_Type, color = Item_Type)) + geom_point() + scale_x_continuous("Item Visibility", breaks = seq(0,0.35,0.05))+ scale_y_continuous("Item MRP", breaks = seq(0,270,by = 30))+ theme_bw() 
```

Now, we can view a third variable also in same chart, say a categorical variable (Item_Type) which will give the characteristic (item_type) of each data set. Different categories are depicted by way of different color for item_type in below chart.

We can even make it more visually clear by creating separate scatter plots for each separate Item_Type as shown below.

```{r, echo=TRUE}
ggplot(bigMart, aes(Item_Visibility, Item_MRP)) + geom_point(aes(color = Item_Type)) + 
  scale_x_continuous("Item Visibility", breaks = seq(0,0.35,0.05))+
  scale_y_continuous("Item MRP", breaks = seq(0,270,by = 30))+
  theme_bw() + labs(title="Scatterplot") + facet_wrap(~ Item_Type)
```


2. Histogram

When to use: Histogram is used to plot continuous variable. It breaks the data into bins and shows frequency distribution of these bins. We can always change the bin size and see the effect it has on visualization.

From our mart dataset, if we want to know the count of items on basis of their cost, then we can plot histogram using continuous variable Item_MRP as shown below.

```{r}
ggplot(bigMart, aes(Item_MRP)) + geom_histogram(binwidth = 2)+
  scale_x_continuous("Item MRP", breaks = seq(0,270,by = 30))+
  scale_y_continuous("Count", breaks = seq(0,200,by = 20))+
  labs(title = "Histogram")
```

3. Bar & Stack Bar Chart

**When to use:** Bar charts are recommended when you want to plot a categorical variable or a combination of continuous and categorical variable.

From our dataset, if we want to know number of marts established in particular year, then bar chart would be most suitable option, use variable Establishment Year as shown below.

```{r}
ggplot(bigMart, aes(Outlet_Establishment_Year)) + geom_bar(fill = "red")+theme_bw()+
  scale_x_continuous("Establishment Year", breaks = seq(1985,2010)) + 
  scale_y_continuous("Count", breaks = seq(0,1500,150)) +
  coord_flip()+ labs(title = "Bar Chart") + theme_gray()

```



```{r}
ggplot(bigMart, aes(Item_Type, Item_Weight)) + geom_bar(stat = "identity", fill = "darkblue") + scale_x_discrete("Outlet Type")+ scale_y_continuous("Item Weight", breaks = seq(0,15000, by = 500))+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + labs(title = "Bar Chart")
```
Stacked Bar

```{r}
ggplot(bigMart, aes(Outlet_Location_Type, fill = Outlet_Type)) + geom_bar()+
labs(title = "Stacked Bar Chart", x = "Outlet Location Type", y = "Count of Outlets")
```

4. Box Plot
When to use: Box Plots are used to plot a combination of categorical and continuous variables. This plot is useful for visualizing the spread of the data and detect outliers. It shows five statistically significant numbers- the minimum, the 25th percentile, the median, the 75th percentile and the maximum.

From our dataset, if we want to identify each outlet’s detailed item sales including minimum, maximum & median numbers, box plot can be helpful. In addition, it also gives values of outliers of item sales for each outlet as shown in below chart.

The black points are outliers. Outlier detection and removal is an essential step of successful data exploration.

```{r}
ggplot(bigMart, aes(Outlet_Identifier, Item_Outlet_Sales)) + geom_boxplot(fill = "red")+
scale_y_continuous("Item Outlet Sales", breaks= seq(0,15000, by=500))+
labs(title = "Box Plot", x = "Outlet Identifier")

```

5. Area Chart
When to use: Area chart is used to show continuity across a variable or data set. It is very much same as line chart and is commonly used for time series plots. Alternatively, it is also used to plot continuous variables and analyze the underlying trends.

From our dataset, when we want to analyze the trend of item outlet sales, area chart can be plotted as shown below. It shows count of outlets on basis of sales.

```{r}
ggplot(bigMart, aes(Item_Outlet_Sales)) + geom_area(stat = "bin", bins = 30, fill = "steelblue") + scale_x_continuous(breaks = seq(0, 11000, 1000)) + labs(title = "Area Chart", x = "Item Outlet Sales", y = "Number of Outlets")
```

6. Heat Map
When to use: Heat Map uses intensity (density) of colors to display relationship between two or three or many variables in a two dimensional image. It allows you to explore two dimensions as the axis and the third dimension by intensity of color.

From our dataset, if we want to know cost of each item on every outlet, we can plot heatmap as shown below using three variables Item MRP, Outlet Identifier & Item Type from our mart dataset.

```{r}
ggplot(bigMart, aes(Outlet_Identifier, Item_Type)) + geom_raster(aes(fill = Item_MRP)) + labs(title = "Heat Map", x = "Outlet Identifier", y = "Item Type") + scale_fill_continuous(name = "Item MRP")
```

7. Correlogram
When to use: Correlogram is used to test the level of co-relation among the variable available in the data set. The cells of the matrix can be shaded or colored to show the co-relation value.

Darker the color, higher the co-relation between variables. Positive co-relations are displayed in blue and negative correlations in red color. Color intensity is proportional to the co-relation value.

From our dataset, let’s check co-relation between Item cost, weight, visibility along with Outlet establishment year and Outlet sales from below plot.

In our example, we can see that Item cost & Outlet sales are positively correlated while Item weight & its visibility are negatively correlated.

```{r}
library(corrgram)
corrgram(bigMart, order = NULL, panel = panel.shade, text.panel = panel.txt, main = "Correlogram")
```

[Another Correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram)

```{r}
head(mtcars)
#find correlations
M <- cor(mtcars)
head(M)
```
```{r}
#Visualizing the correlation matrix
library(corrplot)
corrplot(M, method = "circle")
corrplot(M, method = "number")
corrplot(M, type="upper", order="hclust", col=c("black", "white"), bg="lightblue", tl.col="black", tl.srt=45)
```

```{r}
# mat : is a matrix of data
# ... : further arguments to pass to the native R cor.test function
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(mtcars)
head(p.mat[, 1:5])
```
Add significance level to the correlogram

```{r}
# Specialized the insignificant value according to the significant level
corrplot(M, type="upper", order="hclust", 
         p.mat = p.mat, sig.level = 0.01)
```

```{r}
# Leave blank on no significant coefficient
corrplot(M, type="upper", order="hclust", 
         p.mat = p.mat, sig.level = 0.01, insig = "blank")
```

```{r}
# library
library(ggplot2)
 

# Datasets
prc <- read.csv("http://ichart.finance.yahoo.com/table.csv?s=^GSPC&d=0&e=1&f=2017&g=m&a=0&b=1&c=1990&ignore=.csv", as.is=T)
vix <- read.csv("http://ichart.finance.yahoo.com/table.csv?s=%5EVIX&a=00&b=2&c=1990&d=0&e=1&f=2017&g=m&ignore=.csv", as.is=T)
 
head(df)
dim(prc)
# Data processing
prc$Date <- as.Date(prc$Date)
prc <- prc[, c(1,7)]
colnames(prc)[2] <-c("Value")
 
vix$Date <- as.Date(vix$Date)
vix <- vix[, c(1,5)]
colnames(vix)[2] <-c("VIX")
 
df <- merge(prc, vix)
df$year <- as.integer(substring(df$Date,1,4))
df$month <- as.integer(substring(df$Date,6,7))
 
# Graphs
par(mfrow=c(2,1))
plot(df$Date, df$Value, type="l",main="S&P500",  xlab="", ylab="")
plot(df$Date, df$VIX, type="l",main="VIX ( VOLATILITY S&P 500) ",  xlab="", ylab="")
 
# Erase
frame()
par(mfrow=c(1,1)) 
 
# ggplot2 base layer
p <- ggplot(df)
 
# Line graph
(p + geom_line(aes(x=Date, y=Value, colour=VIX)) +
    scale_colour_gradient(low="blue", high="red") + theme_bw()
)
 
# Bubble plots
(p + geom_point(aes(x = month, y = year, size = Value, colour = VIX),shape=16, alpha=0.80) +
    scale_colour_gradient(limits = c(10, 60), low="blue", high="red", breaks= seq(10, 60, by = 10))  +
    scale_x_continuous(breaks = 1:12, labels=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")) +
    scale_y_continuous(trans = "reverse")
)
 
# fin.
```


```{r}
library(plotly)
data <- read.csv("https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv")
head(data)
p <- plot_ly(data, x = ~gdpPercap, y = ~pop, text = ~gdpPercap, type = 'scatter', mode = 'markers',
        marker = list(size = ~country, opacity = 0.09, color = ~country)) %>%
  layout(title = 'Gender Gap in Earnings per University',
         xaxis = list(showgrid = FALSE),
         yaxis = list(showgrid = FALSE))
p

```

```{r}
library(ggplot2)
set.seed(53)
x_var <- rnorm( n = 15, mean = 5, sd = 2)
y_var <- x_var + rnorm(n = 15, mean = 5, sd =4)
size_var <- runif(15, 1,10)
sex <- rep(c("male", "male", "female"), each = 5)
sex <-replicate(5, sex, FALSE)
sex
df.test_data <- cbind.data.frame(x_var, y_var, size_var, sex)
df.test_data

```

```{r}
# PLOT THE DATA USING GGPLOT2
ggplot(data=df.test_data, aes(x=x_var, y=y_var)) +
  geom_point(aes(size=size_var, color = sex)) + #this makes it a bubble plot
  scale_size_continuous(range=c(2,15)) +
  theme(legend.position = "none")+ scale_color_manual(values=c("#CC6666", "#9999CC"))
```

```{r}

```

